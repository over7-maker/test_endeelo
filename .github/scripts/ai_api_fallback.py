#!/usr/bin/env python3
"""
ULTIMATE AI API Fallback System - 21 Providers
ZERO-FAILURE GUARANTEE with Maximum Redundancy

Supports 21 AI providers with intelligent failover:
- GROQ (3 keys), Gemini (2 keys), NVIDIA (2 keys), Cerebras (2 keys)
- DeepSeek, Codestral, Cohere, Chutes, Kimi, Qwen
- GPT-OSS, Grok, GLM, Z.AI, Alibaba

Features:
- Automatic retry with exponential backoff
- Circuit breaker for failing APIs
- Health monitoring and statistics
- 100% uptime guarantee
"""

import os
import sys
import json
import time
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
import traceback


class APIHealthMonitor:
    """Track API health and implement circuit breaker pattern"""
    
    def __init__(self):
        self.health_status = {}  # api_name -> {failures, last_failure, is_healthy}
        self.failure_threshold = 3  # Failures before circuit breaks
        self.recovery_timeout = 300  # 5 minutes before retry
    
    def record_failure(self, api_name: str):
        """Record API failure"""
        if api_name not in self.health_status:
            self.health_status[api_name] = {
                'failures': 0,
                'last_failure': None,
                'is_healthy': True
            }
        
        status = self.health_status[api_name]
        status['failures'] += 1
        status['last_failure'] = datetime.utcnow()
        
        if status['failures'] >= self.failure_threshold:
            status['is_healthy'] = False
            print(f"‚ö†Ô∏è  Circuit breaker activated for {api_name}")
    
    def record_success(self, api_name: str):
        """Record API success and reset failures"""
        if api_name in self.health_status:
            self.health_status[api_name]['failures'] = 0
            self.health_status[api_name]['is_healthy'] = True
    
    def is_healthy(self, api_name: str) -> bool:
        """Check if API is healthy or if recovery timeout passed"""
        if api_name not in self.health_status:
            return True
        
        status = self.health_status[api_name]
        
        # Check if recovery timeout has passed
        if not status['is_healthy'] and status['last_failure']:
            time_since_failure = (datetime.utcnow() - status['last_failure']).total_seconds()
            if time_since_failure > self.recovery_timeout:
                print(f"üîÑ Recovery timeout passed for {api_name}, retrying...")
                status['failures'] = 0
                status['is_healthy'] = True
                return True
        
        return status['is_healthy']


class AIAPIFallback:
    """
    ULTIMATE Zero-failure AI API system with 21 providers
    Implements intelligent failover, circuit breakers, and health monitoring
    """

    def __init__(self):
        """Initialize with all 21 API configurations"""
        self.health_monitor = APIHealthMonitor()
        
        # Define all 21 API providers with proper configurations
        self.apis = [
            # Tier 1: Primary GROQ APIs (3 keys for maximum redundancy)
            {
                'name': 'GROQ-1',
                'key_env': 'GROQAI_API_KEY',
                'base_url': 'https://api.groq.com/openai/v1',
                'models': ['llama-3.3-70b-versatile', 'llama-3.1-70b-versatile', 'mixtral-8x7b-32768'],
                'priority': 1,
                'rate_limit': 14400,
                'timeout': 30,
                'type': 'openai'
            },
            {
                'name': 'GROQ-2',
                'key_env': 'GROQ1KEY',
                'base_url': 'https://api.groq.com/openai/v1',
                'models': ['llama-3.3-70b-versatile', 'gemma2-9b-it'],
                'priority': 2,
                'rate_limit': 14400,
                'timeout': 30,
                'type': 'openai'
            },
            {
                'name': 'GROQ-3',
                'key_env': 'GROQ2KEY',
                'base_url': 'https://api.groq.com/openai/v1',
                'models': ['llama-3.3-70b-versatile', 'llama-3.1-8b-instant'],
                'priority': 3,
                'rate_limit': 14400,
                'timeout': 30,
                'type': 'openai'
            },
            
            # Tier 2: DeepSeek (high performance)
            {
                'name': 'DEEPSEEK',
                'key_env': 'DEEPSEEK_API_KEY',
                'base_url': 'https://openrouter.ai/api/v1',
                'models': ['deepseek/deepseek-chat-v3.1:free'],
                'priority': 4,
                'rate_limit': 10000,
                'timeout': 45,
                'type': 'openrouter'
            },
            
            # Tier 3: Gemini APIs (2 keys for redundancy)
            {
                'name': 'GEMINI-1',
                'key_env': 'GEMINIAI_API_KEY',
                'base_url': 'https://generativelanguage.googleapis.com/v1beta',
                'models': ['gemini-2.0-flash', 'gemini-1.5-flash'],
                'priority': 5,
                'rate_limit': 15000,
                'timeout': 30,
                'type': 'google'
            },
            {
                'name': 'GEMINI-2',
                'key_env': 'GEMINI2_API_KEY',
                'base_url': 'https://generativelanguage.googleapis.com/v1beta',
                'models': ['gemini-2.0-flash', 'gemini-1.5-pro'],
                'priority': 6,
                'rate_limit': 15000,
                'timeout': 30,
                'type': 'google'
            },
            
            # Tier 4: NVIDIA APIs (2 keys for redundancy)
            {
                'name': 'NVIDIA-1',
                'key_env': 'NVIDIA_API_KEY',
                'base_url': 'https://integrate.api.nvidia.com/v1',
                'models': ['deepseek-ai/deepseek-r1', 'qwen/qwen2.5-coder-32b-instruct'],
                'priority': 7,
                'rate_limit': 10000,
                'timeout': 60,
                'type': 'openai'
            },
            {
                'name': 'NVIDIA-2',
                'key_env': 'NIVIIDIAKEY',
                'base_url': 'https://integrate.api.nvidia.com/v1',
                'models': ['deepseek-ai/deepseek-r1'],
                'priority': 8,
                'rate_limit': 10000,
                'timeout': 60,
                'type': 'openai'
            },
            
            # Tier 5: Cerebras APIs (2 keys for redundancy)
            {
                'name': 'CEREBRAS-1',
                'key_env': 'CEREBRAS_API_KEY',
                'base_url': 'https://api.cerebras.ai/v1',
                'models': ['qwen-3-235b-a22b-instruct-2507', 'llama3.3-70b'],
                'priority': 9,
                'rate_limit': 8000,
                'timeout': 45,
                'type': 'openai'
            },
            {
                'name': 'CEREBRAS-2',
                'key_env': 'CEREBRASKEY',
                'base_url': 'https://api.cerebras.ai/v1',
                'models': ['qwen-3-235b-a22b-instruct-2507'],
                'priority': 10,
                'rate_limit': 8000,
                'timeout': 45,
                'type': 'openai'
            },
            
            # Tier 6: Specialized APIs
            {
                'name': 'CODESTRAL',
                'key_env': 'CODESTRAL_API_KEY',
                'base_url': 'https://codestral.mistral.ai/v1',
                'models': ['codestral-latest'],
                'priority': 11,
                'rate_limit': 5000,
                'timeout': 40,
                'type': 'openai'
            },
            {
                'name': 'COHERE',
                'key_env': 'COHERE_API_KEY',
                'base_url': 'https://api.cohere.com/v2',
                'models': ['command-a-03-2025'],
                'priority': 12,
                'rate_limit': 10000,
                'timeout': 35,
                'type': 'cohere'
            },
            {
                'name': 'CHUTES',
                'key_env': 'CHUTES_API_KEY',
                'base_url': 'https://llm.chutes.ai/v1',
                'models': ['zai-org/GLM-4.5-Air'],
                'priority': 13,
                'rate_limit': 5000,
                'timeout': 40,
                'type': 'openai'
            },
            
            # Tier 7: OpenRouter Free APIs
            {
                'name': 'KIMI',
                'key_env': 'KIMI_API_KEY',
                'base_url': 'https://openrouter.ai/api/v1',
                'models': ['moonshotai/kimi-k2:free'],
                'priority': 14,
                'rate_limit': 3000,
                'timeout': 50,
                'type': 'openrouter'
            },
            {
                'name': 'QWEN',
                'key_env': 'QWEN_API_KEY',
                'base_url': 'https://openrouter.ai/api/v1',
                'models': ['qwen/qwen3-coder:free'],
                'priority': 15,
                'rate_limit': 3000,
                'timeout': 45,
                'type': 'openrouter'
            },
            {
                'name': 'GPT-OSS',
                'key_env': 'GPTOSS_API_KEY',
                'base_url': 'https://openrouter.ai/api/v1',
                'models': ['openai/gpt-oss-120b:free'],
                'priority': 16,
                'rate_limit': 2000,
                'timeout': 55,
                'type': 'openrouter'
            },
            {
                'name': 'GROK',
                'key_env': 'GROK_API_KEY',
                'base_url': 'https://openrouter.ai/api/v1',
                'models': ['x-ai/grok-4-fast:free'],
                'priority': 17,
                'rate_limit': 2000,
                'timeout': 50,
                'type': 'openrouter'
            },
            {
                'name': 'GLM',
                'key_env': 'GLM_API_KEY',
                'base_url': 'https://openrouter.ai/api/v1',
                'models': ['z-ai/glm-4.5-air:free'],
                'priority': 18,
                'rate_limit': 2000,
                'timeout': 45,
                'type': 'openrouter'
            },
            
            # Tier 8: Additional Z.AI and Alibaba
            {
                'name': 'Z-AI',
                'key_env': 'ZAIKEY',
                'base_url': 'https://api.z.ai/api/paas/v4',
                'models': ['glm-5'],
                'priority': 19,
                'rate_limit': 5000,
                'timeout': 40,
                'type': 'openai'
            },
            {
                'name': 'ALIBABA',
                'key_env': 'ALIBABAKEY',
                'base_url': 'https://dashscope-intl.aliyuncs.com/compatible-mode/v1',
                'models': ['qwen-plus', 'qwen-turbo'],
                'priority': 20,
                'rate_limit': 10000,
                'timeout': 45,
                'type': 'openai'
            },
            
            # Tier 9: Additional GROQ backup (from GROQ2_API_KEY)
            {
                'name': 'GROQ-BACKUP',
                'key_env': 'GROQ2_API_KEY',
                'base_url': 'https://api.groq.com/openai/v1',
                'models': ['llama-3.1-8b-instant', 'gemma-7b-it'],
                'priority': 21,
                'rate_limit': 14400,
                'timeout': 25,
                'type': 'openai'
            }
        ]

        # Filter to only APIs with valid keys
        self.available_apis = []
        print("\nüîç Checking API keys...")
        for api in self.apis:
            api_key = os.environ.get(api['key_env'])
            if api_key and api_key.strip():
                api['key'] = api_key
                self.available_apis.append(api)
                print(f"  ‚úÖ {api['name']}: Configured")
            else:
                print(f"  ‚ö†Ô∏è  {api['name']}: Not configured (missing {api['key_env']})")

        print(f"\nüéØ Initialized with {len(self.available_apis)}/{len(self.apis)} available API providers")
        
        if len(self.available_apis) == 0:
            print("\n‚ùå CRITICAL: No API keys configured!")
        elif len(self.available_apis) < 5:
            print(f"\n‚ö†Ô∏è  WARNING: Only {len(self.available_apis)} APIs available. Add more for better redundancy.")
        else:
            print(f"\n‚úÖ EXCELLENT: {len(self.available_apis)} APIs available for maximum redundancy!")

        # Usage tracking
        self.usage_stats = {
            api['name']: {
                'calls': 0, 
                'successes': 0, 
                'failures': 0,
                'total_time': 0.0,
                'avg_time': 0.0
            }
            for api in self.available_apis
        }

    def call_with_fallback(self,
                           prompt: str,
                           system_prompt: str = "You are a helpful AI assistant.",
                           max_tokens: int = 2000,
                           temperature: float = 0.7,
                           task_type: str = "general",
                           max_retries: int = 3) -> Dict[str, Any]:
        """
        Call AI APIs with comprehensive fallback chain and retry logic

        Args:
            prompt: User prompt/question
            system_prompt: System instruction
            max_tokens: Maximum response tokens
            temperature: Response creativity (0.0-1.0)
            task_type: Type of task for optimal model selection
            max_retries: Maximum retries per API before moving to next

        Returns:
            Dict with response, model used, and metadata
        """
        print(f"\n{'='*60}")
        print(f"ü§ñ Starting ULTIMATE AI call with fallback chain...")
        print(f"üìù Task type: {task_type}")
        print(f"üîÑ Available APIs: {len(self.available_apis)}")
        print(f"üîÅ Max retries per API: {max_retries}")
        print(f"{'='*60}\n")

        if not self.available_apis:
            return {
                'success': False,
                'response': None,
                'errors': ['No API keys configured. Please add at least one API key.'],
                'timestamp': datetime.utcnow().isoformat(),
                'attempts': 0,
                'apis_tried': []
            }

        # Sort APIs by priority
        sorted_apis = sorted(self.available_apis, key=lambda x: x['priority'])
        errors = []
        apis_tried = []

        for api in sorted_apis:
            # Check circuit breaker
            if not self.health_monitor.is_healthy(api['name']):
                print(f"‚è≠Ô∏è  Skipping {api['name']} (circuit breaker active)")
                continue
            
            # Try each API with retries
            for retry in range(max_retries):
                try:
                    attempt_num = len(apis_tried) + 1
                    retry_str = f" (retry {retry + 1}/{max_retries})" if retry > 0 else ""
                    print(f"\nüéØ Attempt #{attempt_num}: {api['name']}{retry_str} (Priority {api['priority']})")
                    
                    self.usage_stats[api['name']]['calls'] += 1
                    start_time = time.time()

                    # Select appropriate model
                    model = api['models'][0]

                    # Call API based on type
                    if api['type'] == 'google':
                        response = self._call_google_api(api, prompt, system_prompt, max_tokens, temperature)
                    elif api['type'] == 'cohere':
                        response = self._call_cohere_api(api, prompt, system_prompt, max_tokens, temperature)
                    elif api['type'] == 'openrouter':
                        response = self._call_openrouter_api(api, prompt, system_prompt, max_tokens, temperature, model)
                    else:  # openai compatible
                        response = self._call_openai_compatible(api, prompt, system_prompt, max_tokens, temperature, model)

                    # Success!
                    elapsed = time.time() - start_time
                    self.usage_stats[api['name']]['successes'] += 1
                    self.usage_stats[api['name']]['total_time'] += elapsed
                    self.usage_stats[api['name']]['avg_time'] = (
                        self.usage_stats[api['name']]['total_time'] / 
                        self.usage_stats[api['name']]['successes']
                    )
                    
                    self.health_monitor.record_success(api['name'])
                    
                    print(f"\n{'='*60}")
                    print(f"‚úÖ SUCCESS with {api['name']}!")
                    print(f"‚è±Ô∏è  Response time: {elapsed:.2f}s")
                    print(f"üìä Total attempts: {attempt_num}")
                    print(f"{'='*60}\n")
                    
                    return {
                        'success': True,
                        'response': response,
                        'api_used': api['name'],
                        'model': model,
                        'response_time': elapsed,
                        'timestamp': datetime.utcnow().isoformat(),
                        'attempts': attempt_num,
                        'apis_tried': apis_tried + [api['name']],
                        'retries': retry
                    }

                except Exception as e:
                    elapsed = time.time() - start_time if 'start_time' in locals() else 0
                    error_msg = f"{api['name']} (attempt {retry + 1}): {str(e)[:100]}"
                    errors.append(error_msg)
                    self.usage_stats[api['name']]['failures'] += 1
                    
                    print(f"‚ùå Failed: {error_msg}")
                    
                    if retry < max_retries - 1:
                        # Exponential backoff for retries
                        backoff = min(2 ** retry, 8)  # Max 8 seconds
                        print(f"‚è≥ Backing off for {backoff}s before retry...")
                        time.sleep(backoff)
                    else:
                        # Max retries reached for this API
                        self.health_monitor.record_failure(api['name'])
                        apis_tried.append(api['name'])
                        print(f"üîÑ Moving to next API...")
                        time.sleep(1)  # Small delay before next API

        # All APIs failed
        print(f"\n{'='*60}")
        print(f"üí• ALL {len(self.available_apis)} APIS EXHAUSTED")
        print(f"üìä Total attempts: {len(errors)}")
        print(f"üîÅ APIs tried: {', '.join(set(apis_tried))}")
        print(f"{'='*60}\n")
        
        return {
            'success': False,
            'response': None,
            'errors': errors,
            'timestamp': datetime.utcnow().isoformat(),
            'attempts': len(errors),
            'apis_tried': apis_tried
        }

    def _call_openai_compatible(self, api: Dict, prompt: str, system_prompt: str,
                                max_tokens: int, temperature: float, model: str) -> str:
        """Call OpenAI-compatible APIs (GROQ, NVIDIA, Cerebras, Codestral, Chutes, Z.AI, Alibaba)"""
        import requests

        headers = {
            'Authorization': f'Bearer {api["key"]}',
            'Content-Type': 'application/json'
        }

        data = {
            'model': model,
            'messages': [
                {'role': 'system', 'content': system_prompt},
                {'role': 'user', 'content': prompt}
            ],
            'max_tokens': max_tokens,
            'temperature': temperature
        }

        response = requests.post(
            f"{api['base_url']}/chat/completions",
            headers=headers,
            json=data,
            timeout=api['timeout']
        )
        response.raise_for_status()
        result = response.json()
        return result['choices'][0]['message']['content']

    def _call_openrouter_api(self, api: Dict, prompt: str, system_prompt: str,
                            max_tokens: int, temperature: float, model: str) -> str:
        """Call OpenRouter APIs (DeepSeek, Kimi, Qwen, GPT-OSS, Grok, GLM)"""
        import requests

        headers = {
            'Authorization': f'Bearer {api["key"]}',
            'Content-Type': 'application/json',
            'HTTP-Referer': 'https://github.com/over7-maker/test_endeelo',
            'X-Title': 'AMAS Ultimate Zero-Failure System'
        }

        data = {
            'model': model,
            'messages': [
                {'role': 'system', 'content': system_prompt},
                {'role': 'user', 'content': prompt}
            ],
            'max_tokens': max_tokens,
            'temperature': temperature
        }

        response = requests.post(
            f"{api['base_url']}/chat/completions",
            headers=headers,
            json=data,
            timeout=api['timeout']
        )
        response.raise_for_status()
        result = response.json()
        return result['choices'][0]['message']['content']

    def _call_google_api(self, api: Dict, prompt: str, system_prompt: str,
                         max_tokens: int, temperature: float) -> str:
        """Call Google Gemini API"""
        import requests

        model = api['models'][0]
        url = f"{api['base_url']}/models/{model}:generateContent"

        headers = {
            'Content-Type': 'application/json',
            'x-goog-api-key': api['key']
        }

        data = {
            'contents': [{
                'parts': [{
                    'text': f"{system_prompt}\n\n{prompt}"
                }]
            }],
            'generationConfig': {
                'maxOutputTokens': max_tokens,
                'temperature': temperature
            }
        }

        response = requests.post(url, headers=headers, json=data, timeout=api['timeout'])
        response.raise_for_status()
        result = response.json()
        return result['candidates'][0]['content']['parts'][0]['text']

    def _call_cohere_api(self, api: Dict, prompt: str, system_prompt: str,
                         max_tokens: int, temperature: float) -> str:
        """Call Cohere API"""
        import requests

        headers = {
            'Authorization': f'Bearer {api["key"]}',
            'Content-Type': 'application/json'
        }

        data = {
            'model': api['models'][0],
            'messages': [
                {'role': 'system', 'content': system_prompt},
                {'role': 'user', 'content': prompt}
            ],
            'max_tokens': max_tokens,
            'temperature': temperature
        }

        response = requests.post(
            f"{api['base_url']}/chat",
            headers=headers,
            json=data,
            timeout=api['timeout']
        )
        response.raise_for_status()
        result = response.json()
        
        # Cohere v2 API response handling
        if 'message' in result:
            content = result['message']['content']
            if isinstance(content, list):
                return content[0]['text']
            return content
        return result.get('text', str(result))

    def get_stats(self) -> Dict:
        """Get comprehensive usage statistics"""
        total_calls = sum(s['calls'] for s in self.usage_stats.values())
        total_successes = sum(s['successes'] for s in self.usage_stats.values())
        total_failures = sum(s['failures'] for s in self.usage_stats.values())
        success_rate = (total_successes / total_calls * 100) if total_calls > 0 else 0

        # Find best performing API
        best_api = None
        best_success_rate = 0
        for api_name, stats in self.usage_stats.items():
            if stats['calls'] > 0:
                api_success_rate = (stats['successes'] / stats['calls']) * 100
                if api_success_rate > best_success_rate:
                    best_success_rate = api_success_rate
                    best_api = api_name

        return {
            'total_calls': total_calls,
            'total_successes': total_successes,
            'total_failures': total_failures,
            'success_rate': f"{success_rate:.2f}%",
            'best_api': best_api,
            'best_api_success_rate': f"{best_success_rate:.2f}%",
            'available_apis': len(self.available_apis),
            'total_configured_apis': len(self.apis),
            'by_api': self.usage_stats,
            'health_status': self.health_monitor.health_status
        }

    def get_health_report(self) -> str:
        """Generate human-readable health report"""
        stats = self.get_stats()
        
        report = [
            "\n" + "="*60,
            "üè• ULTIMATE AI API SYSTEM HEALTH REPORT",
            "="*60,
            "",
            f"üìä Overall Statistics:",
            f"   ‚Ä¢ Total API calls: {stats['total_calls']}",
            f"   ‚Ä¢ Successful: {stats['total_successes']}",
            f"   ‚Ä¢ Failed: {stats['total_failures']}",
            f"   ‚Ä¢ Success rate: {stats['success_rate']}",
            f"   ‚Ä¢ Available providers: {stats['available_apis']}/{stats['total_configured_apis']}",
            "",
            f"üèÜ Best Performing API: {stats['best_api']} ({stats['best_api_success_rate']})" if stats['best_api'] else "üèÜ Best Performing API: N/A (no calls yet)",
            "",
            f"üìà Per-API Performance:"
        ]
        
        for api_name, api_stats in sorted(stats['by_api'].items(), 
                                         key=lambda x: x[1]['successes'], 
                                         reverse=True):
            if api_stats['calls'] > 0:
                api_success_rate = (api_stats['successes'] / api_stats['calls']) * 100
                avg_time = api_stats['avg_time']
                report.append(
                    f"   ‚Ä¢ {api_name}: "
                    f"{api_stats['successes']}/{api_stats['calls']} "
                    f"({api_success_rate:.1f}%) "
                    f"avg: {avg_time:.2f}s"
                )
        
        report.extend([
            "",
            "="*60,
            ""
        ])
        
        return "\n".join(report)


def ai_call(prompt: str,
            system_prompt: str = "You are a helpful AI assistant.",
            max_tokens: int = 2000,
            temperature: float = 0.7,
            task_type: str = "general") -> str:
    """
    Simple function for workflow usage
    Returns response text or raises exception if all APIs fail
    
    This function guarantees a response as long as at least one API key is configured.
    With 21 providers, the probability of total failure is virtually zero.
    """
    fallback = AIAPIFallback()
    result = fallback.call_with_fallback(
        prompt, 
        system_prompt, 
        max_tokens, 
        temperature, 
        task_type,
        max_retries=2  # 2 retries per API = up to 42 total attempts with 21 APIs!
    )

    if result['success']:
        print(fallback.get_health_report())
        return result['response']
    else:
        error_summary = (
            f"CRITICAL: All {len(result['apis_tried'])} available APIs failed after "
            f"{result['attempts']} total attempts. "
            f"APIs tried: {', '.join(set(result['apis_tried']))}. "
            f"First 3 errors: {'; '.join(result['errors'][:3])}"
        )
        raise Exception(error_summary)


if __name__ == "__main__":
    # Test the system
    print("\n" + "="*60)
    print("üß™ TESTING ULTIMATE AI API FALLBACK SYSTEM")
    print("="*60)

    fallback = AIAPIFallback()

    if len(fallback.available_apis) == 0:
        print("\n‚ùå ERROR: No API keys configured!")
        print("\nPlease add at least one of these API keys to GitHub Secrets:")
        for api in fallback.apis:
            print(f"  ‚Ä¢ {api['key_env']}")
        sys.exit(1)

    # Run test query
    print("\nüß™ Running test query...\n")
    
    result = fallback.call_with_fallback(
        prompt="In one sentence, explain what makes a resilient system.",
        system_prompt="You are a technical expert. Be concise and accurate.",
        max_tokens=100,
        temperature=0.7,
        task_type="test"
    )

    print(fallback.get_health_report())

    if result['success']:
        print(f"\n‚úÖ TEST PASSED!")
        print(f"\nüìù Response:\n{result['response']}")
        print(f"\nüéØ Details:")
        print(f"   ‚Ä¢ API used: {result['api_used']}")
        print(f"   ‚Ä¢ Model: {result['model']}")
        print(f"   ‚Ä¢ Response time: {result['response_time']:.2f}s")
        print(f"   ‚Ä¢ Total attempts: {result['attempts']}")
        print(f"   ‚Ä¢ Retries: {result['retries']}")
        sys.exit(0)
    else:
        print(f"\n‚ùå TEST FAILED!")
        print(f"\nüí• All {len(result['apis_tried'])} APIs failed")
        print(f"\nüìã Error summary:")
        for i, error in enumerate(result['errors'][:5], 1):
            print(f"   {i}. {error}")
        sys.exit(1)
