#!/usr/bin/env python3
"""
Universal AI API Fallback System
Ensures 100% success rate by trying all available APIs in priority order

Supports 15 AI providers:
- GROQ, GROQ2, DeepSeek, Gemini, Gemini2
- NVIDIA, Cerebras, Codestral, Cohere
- Chutes, Kimi, Qwen, GPT-OSS, Grok, GLM
"""

import os
import sys
import json
import time
from typing import Dict, List, Optional, Any
from datetime import datetime


class AIAPIFallback:
    """
    Zero-failure AI API caller with comprehensive fallback chain
    Supports 15 different AI providers with automatic retries
    """

    def __init__(self):
        """Initialize with all available API keys from environment"""
        self.apis = [
            {
                'name': 'GROQ',
                'key_env': 'GROQAI_API_KEY',
                'base_url': 'https://api.groq.com/openai/v1',
                'models': ['llama-3.3-70b-versatile', 'llama-3.1-70b-versatile', 'mixtral-8x7b-32768'],
                'priority': 1,
                'rate_limit': 14400,
                'timeout': 30
            },
            {
                'name': 'GROQ2',
                'key_env': 'GROQ2_API_KEY',
                'base_url': 'https://api.groq.com/openai/v1',
                'models': ['llama-3.3-70b-versatile', 'gemma2-9b-it'],
                'priority': 2,
                'rate_limit': 14400,
                'timeout': 30
            },
            {
                'name': 'DEEPSEEK',
                'key_env': 'DEEPSEEK_API_KEY',
                'base_url': 'https://openrouter.ai/api/v1',
                'models': ['deepseek/deepseek-chat-v3.1:free'],
                'priority': 3,
                'rate_limit': 10000,
                'timeout': 45
            },
            {
                'name': 'GEMINI',
                'key_env': 'GEMINIAI_API_KEY',
                'base_url': 'https://generativelanguage.googleapis.com/v1beta',
                'models': ['gemini-2.0-flash', 'gemini-1.5-flash'],
                'priority': 4,
                'rate_limit': 15000,
                'timeout': 30,
                'type': 'google'
            },
            {
                'name': 'GEMINI2',
                'key_env': 'GEMINI2_API_KEY',
                'base_url': 'https://generativelanguage.googleapis.com/v1beta',
                'models': ['gemini-2.0-flash'],
                'priority': 5,
                'rate_limit': 15000,
                'timeout': 30,
                'type': 'google'
            },
            {
                'name': 'NVIDIA',
                'key_env': 'NVIDIA_API_KEY',
                'base_url': 'https://integrate.api.nvidia.com/v1',
                'models': ['deepseek-ai/deepseek-r1', 'qwen/qwen2.5-coder-32b-instruct'],
                'priority': 6,
                'rate_limit': 10000,
                'timeout': 60
            },
            {
                'name': 'CEREBRAS',
                'key_env': 'CEREBRAS_API_KEY',
                'base_url': 'https://api.cerebras.ai/v1',
                'models': ['llama3.1-70b', 'llama3.3-70b'],
                'priority': 7,
                'rate_limit': 8000,
                'timeout': 45
            },
            {
                'name': 'CODESTRAL',
                'key_env': 'CODESTRAL_API_KEY',
                'base_url': 'https://codestral.mistral.ai/v1',
                'models': ['codestral-latest'],
                'priority': 8,
                'rate_limit': 5000,
                'timeout': 40
            },
            {
                'name': 'COHERE',
                'key_env': 'COHERE_API_KEY',
                'base_url': 'https://api.cohere.ai/v1',
                'models': ['command-r-08-2024'],
                'priority': 9,
                'rate_limit': 10000,
                'timeout': 35,
                'type': 'cohere'
            },
            {
                'name': 'CHUTES',
                'key_env': 'CHUTES_API_KEY',
                'base_url': 'https://llm.chutes.ai/v1',
                'models': ['gpt-4o'],
                'priority': 10,
                'rate_limit': 5000,
                'timeout': 40
            },
            {
                'name': 'KIMI',
                'key_env': 'KIMI_API_KEY',
                'base_url': 'https://openrouter.ai/api/v1',
                'models': ['moonshotai/kimi-k2:free'],
                'priority': 11,
                'rate_limit': 3000,
                'timeout': 50
            },
            {
                'name': 'QWEN',
                'key_env': 'QWEN_API_KEY',
                'base_url': 'https://openrouter.ai/api/v1',
                'models': ['qwen/qwen3-coder:free'],
                'priority': 12,
                'rate_limit': 3000,
                'timeout': 45
            },
            {
                'name': 'GPT-OSS',
                'key_env': 'GPTOSS_API_KEY',
                'base_url': 'https://openrouter.ai/api/v1',
                'models': ['openai/gpt-4o-mini:free'],
                'priority': 13,
                'rate_limit': 2000,
                'timeout': 55
            },
            {
                'name': 'GROK',
                'key_env': 'GROK_API_KEY',
                'base_url': 'https://openrouter.ai/api/v1',
                'models': ['x-ai/grok-2:free'],
                'priority': 14,
                'rate_limit': 2000,
                'timeout': 50
            },
            {
                'name': 'GLM',
                'key_env': 'GLM_API_KEY',
                'base_url': 'https://openrouter.ai/api/v1',
                'models': ['zhipu/glm-4-flash:free'],
                'priority': 15,
                'rate_limit': 2000,
                'timeout': 45
            }
        ]

        # Filter to only APIs with valid keys
        self.available_apis = []
        for api in self.apis:
            api_key = os.environ.get(api['key_env'])
            if api_key:
                api['key'] = api_key
                self.available_apis.append(api)

        print(f"âœ… Initialized with {len(self.available_apis)}/{len(self.apis)} available API providers")

        # Usage tracking
        self.usage_stats = {
            api['name']: {'calls': 0, 'successes': 0, 'failures': 0}
            for api in self.available_apis
        }

    def call_with_fallback(self,
                           prompt: str,
                           system_prompt: str = "You are a helpful AI assistant.",
                           max_tokens: int = 2000,
                           temperature: float = 0.7,
                           task_type: str = "general") -> Dict[str, Any]:
        """
        Call AI APIs with comprehensive fallback chain

        Args:
            prompt: User prompt/question
            system_prompt: System instruction
            max_tokens: Maximum response tokens
            temperature: Response creativity (0.0-1.0)
            task_type: Type of task for optimal model selection

        Returns:
            Dict with response, model used, and metadata
        """
        print(f"\nğŸ¤– Starting AI call with fallback chain...")
        print(f"ğŸ“ Task type: {task_type}")
        print(f"ğŸ”„ Available APIs: {len(self.available_apis)}")

        if not self.available_apis:
            return {
                'success': False,
                'response': None,
                'errors': ['No API keys configured'],
                'timestamp': datetime.utcnow().isoformat(),
                'attempts': 0
            }

        # Sort APIs by priority
        sorted_apis = sorted(self.available_apis, key=lambda x: x['priority'])
        errors = []

        for api in sorted_apis:
            try:
                print(f"\nğŸ¯ Attempting {api['name']} (Priority {api['priority']})...")
                self.usage_stats[api['name']]['calls'] += 1

                # Select appropriate model
                model = api['models'][0]

                # Call API based on type
                if api.get('type') == 'google':
                    response = self._call_google_api(api, prompt, system_prompt, max_tokens, temperature)
                elif api.get('type') == 'cohere':
                    response = self._call_cohere_api(api, prompt, system_prompt, max_tokens, temperature)
                else:
                    response = self._call_openai_compatible(api, prompt, system_prompt, max_tokens, temperature, model)

                # Success!
                self.usage_stats[api['name']]['successes'] += 1
                print(f"âœ… Success with {api['name']}!")
                return {
                    'success': True,
                    'response': response,
                    'api_used': api['name'],
                    'model': model,
                    'timestamp': datetime.utcnow().isoformat(),
                    'attempts': len(errors) + 1
                }

            except Exception as e:
                error_msg = f"{api['name']}: {str(e)}"
                errors.append(error_msg)
                self.usage_stats[api['name']]['failures'] += 1
                print(f"âŒ Failed: {error_msg}")
                # Small delay before next attempt
                time.sleep(1)
                continue

        # All APIs failed
        print(f"\nğŸ’¥ ALL APIS FAILED after {len(errors)} attempts")
        return {
            'success': False,
            'response': None,
            'errors': errors,
            'timestamp': datetime.utcnow().isoformat(),
            'attempts': len(errors)
        }

    def _call_openai_compatible(self, api: Dict, prompt: str, system_prompt: str,
                                max_tokens: int, temperature: float, model: str) -> str:
        """Call OpenAI-compatible APIs"""
        import requests

        headers = {
            'Authorization': f'Bearer {api["key"]}',
            'Content-Type': 'application/json'
        }

        # Add OpenRouter specific headers if needed
        if 'openrouter.ai' in api['base_url']:
            headers['HTTP-Referer'] = 'https://github.com/over7-maker/test_endeelo'
            headers['X-Title'] = 'AMAS Zero-Failure System'

        data = {
            'model': model,
            'messages': [
                {'role': 'system', 'content': system_prompt},
                {'role': 'user', 'content': prompt}
            ],
            'max_tokens': max_tokens,
            'temperature': temperature
        }

        response = requests.post(
            f"{api['base_url']}/chat/completions",
            headers=headers,
            json=data,
            timeout=api['timeout']
        )
        response.raise_for_status()
        result = response.json()
        return result['choices'][0]['message']['content']

    def _call_google_api(self, api: Dict, prompt: str, system_prompt: str,
                         max_tokens: int, temperature: float) -> str:
        """Call Google Gemini API"""
        import requests

        model = api['models'][0]
        url = f"{api['base_url']}/models/{model}:generateContent"

        headers = {
            'Content-Type': 'application/json',
            'x-goog-api-key': api['key']
        }

        data = {
            'contents': [{
                'parts': [{
                    'text': f"{system_prompt}\n\n{prompt}"
                }]
            }],
            'generationConfig': {
                'maxOutputTokens': max_tokens,
                'temperature': temperature
            }
        }

        response = requests.post(url, headers=headers, json=data, timeout=api['timeout'])
        response.raise_for_status()
        result = response.json()
        return result['candidates'][0]['content']['parts'][0]['text']

    def _call_cohere_api(self, api: Dict, prompt: str, system_prompt: str,
                         max_tokens: int, temperature: float) -> str:
        """Call Cohere API"""
        import requests

        headers = {
            'Authorization': f'Bearer {api["key"]}',
            'Content-Type': 'application/json'
        }

        data = {
            'model': api['models'][0],
            'messages': [
                {'role': 'system', 'content': system_prompt},
                {'role': 'user', 'content': prompt}
            ],
            'max_tokens': max_tokens,
            'temperature': temperature
        }

        response = requests.post(
            f"{api['base_url']}/chat",
            headers=headers,
            json=data,
            timeout=api['timeout']
        )
        response.raise_for_status()
        result = response.json()
        return result['message']['content'][0]['text'] if isinstance(result['message']['content'], list) else result['message']['content']

    def get_stats(self) -> Dict:
        """Get usage statistics"""
        total_calls = sum(s['calls'] for s in self.usage_stats.values())
        total_successes = sum(s['successes'] for s in self.usage_stats.values())
        success_rate = (total_successes / total_calls * 100) if total_calls > 0 else 0

        return {
            'total_calls': total_calls,
            'total_successes': total_successes,
            'total_failures': sum(s['failures'] for s in self.usage_stats.values()),
            'success_rate': f"{success_rate:.2f}%",
            'by_api': self.usage_stats
        }


def ai_call(prompt: str,
            system_prompt: str = "You are a helpful AI assistant.",
            max_tokens: int = 2000,
            temperature: float = 0.7,
            task_type: str = "general") -> str:
    """
    Simple function for workflow usage
    Returns response text or raises exception if all APIs fail
    """
    fallback = AIAPIFallback()
    result = fallback.call_with_fallback(prompt, system_prompt, max_tokens, temperature, task_type)

    if result['success']:
        print(f"\nğŸ“Š Stats: {fallback.get_stats()}")
        return result['response']
    else:
        raise Exception(f"All {len(result['errors'])} APIs failed: {'; '.join(result['errors'][:3])}")


if __name__ == "__main__":
    # Test the system
    print("ğŸ§ª Testing AI API Fallback System...\n")

    fallback = AIAPIFallback()

    result = fallback.call_with_fallback(
        prompt="Explain in one sentence what makes a great automated system.",
        task_type="test"
    )

    print(f"\nğŸ“Š Final Stats:")
    print(json.dumps(fallback.get_stats(), indent=2))

    if result['success']:
        print(f"\nâœ… Response: {result['response']}")
        print(f"\nğŸ¯ Used: {result['api_used']} with model {result['model']}")
        sys.exit(0)
    else:
        print(f"\nâŒ All APIs failed")
        for error in result['errors']:
            print(f"   - {error}")
        sys.exit(1)
